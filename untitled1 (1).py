# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H5iHyaMBpc1bXpwRDM4ksNGDASPgJ0Rv
"""

# 1. Install necessary libraries
!pip install transformers[torch] nltk

# 2. Import libraries and download data
import nltk
from transformers import pipeline
import re

# Download the sentence tokenizer model
nltk.download('punkt')

# 3. Define the processing functions
def preprocess_text(text: str) -> str:
    """A simple preprocessing function."""
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    return text

def generate_summary(raw_text: str) -> str:
    """Generates an abstractive summary."""
    # Initialize the summarizer model
    summarizer = pipeline("summarization", model="google/pegasus-cnn_dailymail", device=0) # device=0 uses GPU

    preprocessed_text = preprocess_text(raw_text)

    summary_result = summarizer(
        preprocessed_text,
        min_length=50,
        max_length=1000,
        truncation=True
    )

    return summary_result[0]['summary_text']

# 4. Use the functions
long_text = """
The rapid expansion of digital content across platforms such as news portals, research databases,
educational videos, and social media has significantly increased the demand for automated tools
that can process and distill information efficiently. Abstractive text summarization, an advanced
technique within the field of Natural Language Processing (NLP), aims to generate concise,
human-like summaries of longer texts by understanding and rephrasing the original content.
Unlike extractive summarization, which selects and compiles exact phrases from the source,
abstractive summarization re-generates information using deep learning models, requiring a
deeper semantic understanding.
"""

summary = generate_summary(long_text)
print("--- GENERATED SUMMARY ---")
print(summary)

# Ensure you've run the installations from Method 1 first.

# We are not using argparse in Colab, so we define file paths directly
input_path = "/content/drive/MyDrive/input.txt"
output_path = "summary_output.txt"

print(f"Reading text from: {input_path}")
with open(input_path, 'r', encoding='utf-8') as f:
    source_text = f.read()

print("Generating summary...")
# Using the generate_summary function we defined in the previous cell
summary = generate_summary(source_text)

with open(output_path, 'w', encoding='utf-8') as f:
    f.write(summary)

print(f" Summary successfully saved to: {output_path}")



# 1. Install required libraries
!pip install fastapi uvicorn pyngrok nest-asyncio

# 2. Import libraries
import nest_asyncio
from pyngrok import ngrok
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn
from getpass import getpass

# (Assuming the generate_summary function from the previous method is already defined in your notebook)

# 3. Add your ngrok Authtoken
# This will prompt you to enter your token securely
authtoken = getpass("Enter your ngrok authtoken: ")
ngrok.set_auth_token(authtoken)

# 4. Define the FastAPI app
class TextInput(BaseModel):
    text: str

app = FastAPI()

@app.post("/summarize/")
def summarize_text(payload: TextInput):
    summary = generate_summary(payload.text)
    return {"summary": summary}

# 5. Run the app
nest_asyncio.apply()
public_url = ngrok.connect(8000)
print(f"FastAPI server is running at: {public_url}")
uvicorn.run(app, host="0.0.0.0", port=8000)